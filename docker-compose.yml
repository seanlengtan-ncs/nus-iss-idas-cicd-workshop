services:
  llm-backend:
    build: ./llm-multiroute
    container_name: llm-backend
    ports:
      - "8080:8080"
    env_file:
      - .env
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-https://ollama.com}
      - OLLAMA_TEMPERATURE=${OLLAMA_TEMPERATURE:-0.7}
      - OLLAMA_MODEL_CLASSIFY=${OLLAMA_MODEL_CLASSIFY:-gemma3:4b}
      - OLLAMA_MODEL_SENTIMENT=${OLLAMA_MODEL_SENTIMENT:-ministral-3:3b}
      - OLLAMA_MODEL_SUMMARIZE=${OLLAMA_MODEL_SUMMARIZE:-ministral-3:8b}
      - OLLAMA_MODEL_INTENT=${OLLAMA_MODEL_INTENT:-gemma3:12b}

  llm-frontend:
    build: ./llm-frontend-python
    container_name: llm-frontend
    ports:
      - "5000:5000"
    environment:
      - BACKEND_URL=http://llm-backend:8080
    depends_on:
      - llm-backend